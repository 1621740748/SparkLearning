    [root@sparkonk8s-46929-p4jdl spark-2.2.0-k8s-0.5.0-bin-2.7.3]# ./sbin/build-push-docker-images.sh -r docker.io/myusername -t my-tag build
    Sending build context to Docker daemon 237.4 MB
    Step 1 : FROM openjdk:8-alpine
     ---> 224765a6bdbe
    Step 2 : RUN apk upgrade --no-cache &&     apk add --no-cache bash tini &&     mkdir -p /opt/spark &&     mkdir -p /opt/spark/work-dir     touch /opt/spark/RELEASE &&     rm /bin/sh &&     ln -sv /bin/bash /bin/sh &&     chgrp root /etc/passwd && chmod ug+rw /etc/passwd
     ---> Running in 93472f60c403
    fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz
    fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz
    Upgrading critical system libraries and apk-tools:
    (1/1) Upgrading apk-tools (2.8.2-r0 -> 2.9.1-r0)
    Executing busybox-1.27.2-r7.trigger
    Continuing the upgrade transaction with new apk-tools:
    fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz
    fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz
    (1/4) Upgrading musl (1.1.18-r2 -> 1.1.18-r3)
    (2/4) Upgrading busybox (1.27.2-r7 -> 1.27.2-r8)
    Executing busybox-1.27.2-r8.post-upgrade
    (3/4) Upgrading musl-utils (1.1.18-r2 -> 1.1.18-r3)
    (4/4) Upgrading libtasn1 (4.12-r2 -> 4.12-r3)
    Executing busybox-1.27.2-r8.trigger
    OK: 100 MiB in 51 packages
    fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz
    fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz
    (1/7) Installing pkgconf (1.3.10-r0)
    (2/7) Installing ncurses-terminfo-base (6.0_p20171125-r0)
    (3/7) Installing ncurses-terminfo (6.0_p20171125-r0)
    (4/7) Installing ncurses-libs (6.0_p20171125-r0)
    (5/7) Installing readline (7.0.003-r0)
    (6/7) Installing bash (4.4.19-r1)
    Executing bash-4.4.19-r1.post-install
    (7/7) Installing tini (0.16.1-r0)
    Executing busybox-1.27.2-r8.trigger
    OK: 109 MiB in 58 packages
    '/bin/sh' -> '/bin/bash'
     ---> 7e8af15768af
    Removing intermediate container 93472f60c403
    Step 3 : COPY jars /opt/spark/jars
     ---> 24e67461b3cd
    Removing intermediate container 3a2e33a73b40
    Step 4 : COPY bin /opt/spark/bin
     ---> bce9a7886f7c
    Removing intermediate container 3afe2a0c9450
    Step 5 : COPY sbin /opt/spark/sbin
     ---> 7ae870a76c86
    Removing intermediate container 4987237fa58e
    Step 6 : COPY conf /opt/spark/conf
     ---> 38b39bef9326
    Removing intermediate container 8c0616672fc5
    Step 7 : COPY dockerfiles/spark-base/entrypoint.sh /opt/
     ---> f113b9058d97
    Removing intermediate container e03c3bbd1c6a
    Step 8 : ENV SPARK_HOME /opt/spark
     ---> Running in 0042956bf7a9
     ---> 1ccc5eaf4624
    Removing intermediate container 0042956bf7a9
    Step 9 : WORKDIR /opt/spark/work-dir
     ---> Running in b4ba9248a573
     ---> b7d3614e3a87
    Removing intermediate container b4ba9248a573
    Step 10 : ENTRYPOINT /opt/entrypoint.sh
     ---> Running in eb2f4d7762b9
     ---> def795a30f03
    Removing intermediate container eb2f4d7762b9
    Successfully built def795a30f03
    Sending build context to Docker daemon 237.4 MB
    Step 1 : FROM spark-base
     ---> def795a30f03
    Step 2 : ENTRYPOINT /opt/entrypoint.sh /opt/spark/bin/spark-class org.apache.spark.deploy.k8s.KubernetesExternalShuffleService 1
     ---> Running in bedbb2a9ffbd
     ---> b550f4e45460
    Removing intermediate container bedbb2a9ffbd
    Successfully built b550f4e45460
    Sending build context to Docker daemon 237.4 MB
    Step 1 : FROM spark-base
     ---> def795a30f03
    Step 2 : ENTRYPOINT /opt/entrypoint.sh /opt/spark/bin/spark-class org.apache.spark.deploy.rest.k8s.ResourceStagingServer
     ---> Running in 5d09dee42b3d
     ---> 969e8f41d6bb
    Removing intermediate container 5d09dee42b3d
    Successfully built 969e8f41d6bb
    Sending build context to Docker daemon 237.4 MB
    Step 1 : FROM spark-base
     ---> def795a30f03
    Step 2 : COPY examples /opt/spark/examples
     ---> eb1a94f3b961
    Removing intermediate container da6492646131
    Step 3 : CMD SPARK_CLASSPATH="${SPARK_HOME}/jars/*" &&     env | grep SPARK_JAVA_OPT_ | sed 's/[^=]*=\(.*\)/\1/g' > /tmp/java_opts.txt &&     readarray -t SPARK_DRIVER_JAVA_OPTS < /tmp/java_opts.txt &&     if ! [ -z ${SPARK_MOUNTED_CLASSPATH+x} ]; then SPARK_CLASSPATH="$SPARK_MOUNTED_CLASSPATH:$SPARK_CLASSPATH"; fi &&     if ! [ -z ${SPARK_SUBMIT_EXTRA_CLASSPATH+x} ]; then SPARK_CLASSPATH="$SPARK_SUBMIT_EXTRA_CLASSPATH:$SPARK_CLASSPATH"; fi &&     if ! [ -z ${SPARK_EXTRA_CLASSPATH+x} ]; then SPARK_CLASSPATH="$SPARK_EXTRA_CLASSPATH:$SPARK_CLASSPATH"; fi &&     if ! [ -z ${SPARK_MOUNTED_FILES_DIR+x} ]; then cp -R "$SPARK_MOUNTED_FILES_DIR/." .; fi &&     if ! [ -z ${SPARK_MOUNTED_FILES_FROM_SECRET_DIR} ]; then cp -R "$SPARK_MOUNTED_FILES_FROM_SECRET_DIR/." .; fi &&     ${JAVA_HOME}/bin/java "${SPARK_DRIVER_JAVA_OPTS[@]}" -cp $SPARK_CLASSPATH -Xms$SPARK_DRIVER_MEMORY -Xmx$SPARK_DRIVER_MEMORY -Dspark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS $SPARK_DRIVER_CLASS $SPARK_DRIVER_ARGS
     ---> Running in cc9fbec6e6ff
     ---> c4c89471d132
    Removing intermediate container cc9fbec6e6ff
    Successfully built c4c89471d132
    Sending build context to Docker daemon 237.4 MB
    Step 1 : FROM spark-base
     ---> def795a30f03
    Step 2 : ENTRYPOINT /opt/entrypoint.sh /opt/spark/bin/spark-class org.apache.spark.deploy.rest.k8s.KubernetesSparkDependencyDownloadInitContainer
     ---> Running in 9ca33f415a5e
     ---> 3cd33867b526
    Removing intermediate container 9ca33f415a5e
    Successfully built 3cd33867b526
    Sending build context to Docker daemon 237.4 MB
    Step 1 : FROM spark-base
     ---> def795a30f03
    Step 2 : ADD examples /opt/spark/examples
     ---> 8992a00b93fc
    Removing intermediate container ee352fa92033
    Step 3 : ADD R /opt/spark/R
     ---> 2f473b34f243
    Removing intermediate container 7f3dc0f5ed4e
    Step 4 : RUN apk add --no-cache R R-dev
     ---> Running in ae594cece22c
    fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz
    fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz
    (1/54) Installing R-mathlib (3.4.2-r0)
    (2/54) Installing libice (1.0.9-r2)
    (3/54) Installing libuuid (2.31-r0)
    (4/54) Installing libsm (1.2.2-r1)
    (5/54) Installing libxt (1.1.5-r1)
    (6/54) Installing libxmu (1.1.2-r1)
    (7/54) Installing expat (2.2.5-r0)
    (8/54) Installing fontconfig (2.12.6-r0)
    (9/54) Installing pixman (0.34.0-r3)
    (10/54) Installing cairo (1.14.10-r0)
    (11/54) Installing libssh2 (1.8.0-r2)
    (12/54) Installing libcurl (7.58.0-r1)
    (13/54) Installing libintl (0.19.8.1-r1)
    (14/54) Installing libblkid (2.31-r0)
    (15/54) Installing libmount (2.31-r0)
    (16/54) Installing pcre (8.41-r1)
    (17/54) Installing glib (2.54.2-r0)
    (18/54) Installing libgomp (6.4.0-r5)
    (19/54) Installing icu-libs (59.1-r1)
    (20/54) Installing xz-libs (5.2.3-r1)
    (21/54) Installing libquadmath (6.4.0-r5)
    (22/54) Installing libgfortran (6.4.0-r5)
    (23/54) Installing openblas (0.2.19-r3)
    (24/54) Installing libxft (2.3.2-r2)
    (25/54) Installing graphite2 (1.3.10-r0)
    (26/54) Installing harfbuzz (1.6.3-r0)
    (27/54) Installing pango (1.40.14-r0)
    (28/54) Installing tiff (4.0.9-r1)
    (29/54) Installing R (3.4.2-r0)
    Executing R-3.4.2-r0.post-install
    *
    * If you want to install R packages from CRAN that contains native extensions,
    * then you must also install R-dev.
    *
    (30/54) Installing binutils-libs (2.28-r3)
    (31/54) Installing binutils (2.28-r3)
    (32/54) Installing gmp (6.1.2-r1)
    (33/54) Installing isl (0.18-r0)
    (34/54) Installing libatomic (6.4.0-r5)
    (35/54) Installing mpfr3 (3.1.5-r1)
    (36/54) Installing mpc1 (1.0.3-r1)
    (37/54) Installing gcc (6.4.0-r5)
    (38/54) Installing gfortran (6.4.0-r5)
    (39/54) Installing icu-dev (59.1-r1)
    (40/54) Installing zlib-dev (1.2.11-r1)
    (41/54) Installing libpng-dev (1.6.34-r1)
    (42/54) Installing make (4.2.1-r0)
    (43/54) Installing openblas-ilp64 (0.2.19-r3)
    (44/54) Installing openblas-dev (0.2.19-r3)
    (45/54) Installing libpcre16 (8.41-r1)
    (46/54) Installing libpcre32 (8.41-r1)
    (47/54) Installing libpcrecpp (8.41-r1)
    (48/54) Installing pcre-dev (8.41-r1)
    (49/54) Installing libhistory (7.0.003-r0)
    (50/54) Installing readline-dev (7.0.003-r0)
    (51/54) Installing xz-dev (5.2.3-r1)
    (52/54) Installing bzip2-dev (1.0.6-r6)
    (53/54) Installing curl-dev (7.58.0-r1)
    (54/54) Installing R-dev (3.4.2-r0)
    Executing busybox-1.27.2-r8.trigger
    Executing glib-2.54.2-r0.trigger
    OK: 481 MiB in 112 packages
     ---> 00b3a543e20a
    Removing intermediate container ae594cece22c
    Step 5 : ENV R_HOME /usr/lib/R
     ---> Running in 1bf1111b131c
     ---> 64e045972f97
    Removing intermediate container 1bf1111b131c
    Step 6 : CMD SPARK_CLASSPATH="${SPARK_HOME}/jars/*" &&     env | grep SPARK_JAVA_OPT_ | sed 's/[^=]*=\(.*\)/\1/g' > /tmp/java_opts.txt &&     readarray -t SPARK_DRIVER_JAVA_OPTS < /tmp/java_opts.txt &&     if ! [ -z ${SPARK_MOUNTED_CLASSPATH+x} ]; then SPARK_CLASSPATH="$SPARK_MOUNTED_CLASSPATH:$SPARK_CLASSPATH"; fi &&     if ! [ -z ${SPARK_SUBMIT_EXTRA_CLASSPATH+x} ]; then SPARK_CLASSPATH="$SPARK_SUBMIT_EXTRA_CLASSPATH:$SPARK_CLASSPATH"; fi &&     if ! [ -z ${SPARK_EXTRA_CLASSPATH+x} ]; then SPARK_CLASSPATH="$SPARK_EXTRA_CLASSPATH:$SPARK_CLASSPATH"; fi &&     if ! [ -z ${SPARK_MOUNTED_FILES_DIR+x} ]; then cp -R "$SPARK_MOUNTED_FILES_DIR/." .; fi &&     if ! [ -z ${SPARK_MOUNTED_FILES_FROM_SECRET_DIR+x} ]; then cp -R "$SPARK_MOUNTED_FILES_FROM_SECRET_DIR/." .; fi &&     ${JAVA_HOME}/bin/java "${SPARK_DRIVER_JAVA_OPTS[@]}" -cp $SPARK_CLASSPATH -Xms$SPARK_DRIVER_MEMORY -Xmx$SPARK_DRIVER_MEMORY -Dspark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS $SPARK_DRIVER_CLASS $R_FILE $SPARK_DRIVER_ARGS
     ---> Running in 26a53d3fa248
     ---> 824cb2bd9590
    Removing intermediate container 26a53d3fa248
    Successfully built 824cb2bd9590
    Sending build context to Docker daemon 237.4 MB
    Step 1 : FROM spark-base
     ---> def795a30f03
    Step 2 : ADD examples /opt/spark/examples
     ---> Using cache
     ---> 8992a00b93fc
    Step 3 : ADD python /opt/spark/python
     ---> 7e7980c57484
    Removing intermediate container 017269e2a89a
    Step 4 : RUN apk add --no-cache python &&     python -m ensurepip &&     rm -r /usr/lib/python*/ensurepip &&     pip install --upgrade pip setuptools &&     rm -r /root/.cache
     ---> Running in 2b7be3d11a4f
    fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz
    fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz
    (1/3) Installing expat (2.2.5-r0)
    (2/3) Installing gdbm (1.13-r1)
    (3/3) Installing python2 (2.7.14-r2)
    Executing busybox-1.27.2-r8.trigger
    OK: 147 MiB in 61 packages
    Collecting setuptools
    Collecting pip
    Installing collected packages: setuptools, pip
    Successfully installed pip-9.0.1 setuptools-28.8.0
    Requirement already up-to-date: pip in /usr/lib/python2.7/site-packages
    Collecting setuptools
      Downloading setuptools-38.5.2-py2.py3-none-any.whl (490kB)
    Installing collected packages: setuptools
      Found existing installation: setuptools 28.8.0
        Uninstalling setuptools-28.8.0:
          Successfully uninstalled setuptools-28.8.0
    Successfully installed setuptools-38.5.2
     ---> 3cd82edd4e08
    Removing intermediate container 2b7be3d11a4f
    Step 5 : ENV PYTHON_VERSION 2.7.13
     ---> Running in a95647845775
     ---> 29079fabb899
    Removing intermediate container a95647845775
    Step 6 : ENV PYSPARK_PYTHON python
     ---> Running in cba5eb496f92
     ---> 8cccdec8e336
    Removing intermediate container cba5eb496f92
    Step 7 : ENV PYSPARK_DRIVER_PYTHON python
     ---> Running in 0f8294df0a10
     ---> e4c00176ca31
    Removing intermediate container 0f8294df0a10
    Step 8 : ENV PYTHONPATH ${SPARK_HOME}/python/:${SPARK_HOME}/python/lib/py4j-0.10.4-src.zip:${PYTHONPATH}
     ---> Running in 7edefaa9ebb1
     ---> e84c7e51cc07
    Removing intermediate container 7edefaa9ebb1
    Step 9 : CMD SPARK_CLASSPATH="${SPARK_HOME}/jars/*" &&     env | grep SPARK_JAVA_OPT_ | sed 's/[^=]*=\(.*\)/\1/g' > /tmp/java_opts.txt &&     readarray -t SPARK_DRIVER_JAVA_OPTS < /tmp/java_opts.txt &&     if ! [ -z ${SPARK_MOUNTED_CLASSPATH+x} ]; then SPARK_CLASSPATH="$SPARK_MOUNTED_CLASSPATH:$SPARK_CLASSPATH"; fi &&     if ! [ -z ${SPARK_SUBMIT_EXTRA_CLASSPATH+x} ]; then SPARK_CLASSPATH="$SPARK_SUBMIT_EXTRA_CLASSPATH:$SPARK_CLASSPATH"; fi &&     if ! [ -z ${SPARK_EXTRA_CLASSPATH+x} ]; then SPARK_CLASSPATH="$SPARK_EXTRA_CLASSPATH:$SPARK_CLASSPATH"; fi &&     if ! [ -z ${SPARK_MOUNTED_FILES_DIR+x} ]; then cp -R "$SPARK_MOUNTED_FILES_DIR/." .; fi &&     if ! [ -z ${SPARK_MOUNTED_FILES_FROM_SECRET_DIR+x} ]; then cp -R "$SPARK_MOUNTED_FILES_FROM_SECRET_DIR/." .; fi &&     ${JAVA_HOME}/bin/java "${SPARK_DRIVER_JAVA_OPTS[@]}" -cp $SPARK_CLASSPATH -Xms$SPARK_DRIVER_MEMORY -Xmx$SPARK_DRIVER_MEMORY -Dspark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS $SPARK_DRIVER_CLASS $PYSPARK_PRIMARY $PYSPARK_FILES $SPARK_DRIVER_ARGS
     ---> Running in ae22e6781ede
     ---> f75277e25eae
    Removing intermediate container ae22e6781ede
    Successfully built f75277e25eae
    Sending build context to Docker daemon 237.4 MB
    Step 1 : FROM spark-base
     ---> def795a30f03
    Step 2 : ADD examples /opt/spark/examples
     ---> Using cache
     ---> 8992a00b93fc
    Step 3 : ADD python /opt/spark/python
     ---> Using cache
     ---> 7e7980c57484
    Step 4 : RUN apk add --no-cache python &&     python -m ensurepip &&     rm -r /usr/lib/python*/ensurepip &&     pip install --upgrade pip setuptools &&     rm -r /root/.cache
     ---> Using cache
     ---> 3cd82edd4e08
    Step 5 : ENV PYTHON_VERSION 2.7.13
     ---> Using cache
     ---> 29079fabb899
    Step 6 : ENV PYSPARK_PYTHON python
     ---> Using cache
     ---> 8cccdec8e336
    Step 7 : ENV PYSPARK_DRIVER_PYTHON python
     ---> Using cache
     ---> e4c00176ca31
    Step 8 : ENV PYTHONPATH ${SPARK_HOME}/python/:${SPARK_HOME}/python/lib/py4j-0.10.4-src.zip:${PYTHONPATH}
     ---> Using cache
     ---> e84c7e51cc07
    Step 9 : CMD SPARK_CLASSPATH="${SPARK_HOME}/jars/*" &&     env | grep SPARK_JAVA_OPT_ | sed 's/[^=]*=\(.*\)/\1/g' > /tmp/java_opts.txt &&     readarray -t SPARK_EXECUTOR_JAVA_OPTS < /tmp/java_opts.txt &&     if ! [ -z ${SPARK_MOUNTED_CLASSPATH}+x} ]; then SPARK_CLASSPATH="$SPARK_MOUNTED_CLASSPATH:$SPARK_CLASSPATH"; fi &&     if ! [ -z ${SPARK_EXECUTOR_EXTRA_CLASSPATH+x} ]; then SPARK_CLASSPATH="$SPARK_EXECUTOR_EXTRA_CLASSPATH:$SPARK_CLASSPATH"; fi &&     if ! [ -z ${SPARK_MOUNTED_FILES_DIR+x} ]; then cp -R "$SPARK_MOUNTED_FILES_DIR/." .; fi &&     if ! [ -z ${SPARK_MOUNTED_FILES_FROM_SECRET_DIR+x} ]; then cp -R "$SPARK_MOUNTED_FILES_FROM_SECRET_DIR/." .; fi &&     ${JAVA_HOME}/bin/java "${SPARK_EXECUTOR_JAVA_OPTS[@]}" -Dspark.executor.port=$SPARK_EXECUTOR_PORT -Xms$SPARK_EXECUTOR_MEMORY -Xmx$SPARK_EXECUTOR_MEMORY -cp $SPARK_CLASSPATH org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url $SPARK_DRIVER_URL --executor-id $SPARK_EXECUTOR_ID --cores $SPARK_EXECUTOR_CORES --app-id $SPARK_APPLICATION_ID --hostname $SPARK_EXECUTOR_POD_IP
     ---> Running in 1900db150c8a
     ---> 630ee3e73660
    Removing intermediate container 1900db150c8a
    Successfully built 630ee3e73660
    Sending build context to Docker daemon 237.4 MB
    Step 1 : FROM spark-base
     ---> def795a30f03
    Step 2 : ADD examples /opt/spark/examples
     ---> Using cache
     ---> 8992a00b93fc
    Step 3 : ADD R /opt/spark/R
     ---> Using cache
     ---> 2f473b34f243
    Step 4 : RUN apk add --no-cache R R-dev
     ---> Using cache
     ---> 00b3a543e20a
    Step 5 : ENV R_HOME /usr/lib/R
     ---> Using cache
     ---> 64e045972f97
    Step 6 : CMD SPARK_CLASSPATH="${SPARK_HOME}/jars/*" &&     env | grep SPARK_JAVA_OPT_ | sed 's/[^=]*=\(.*\)/\1/g' > /tmp/java_opts.txt &&     readarray -t SPARK_EXECUTOR_JAVA_OPTS < /tmp/java_opts.txt &&     if ! [ -z ${SPARK_MOUNTED_CLASSPATH}+x} ]; then SPARK_CLASSPATH="$SPARK_MOUNTED_CLASSPATH:$SPARK_CLASSPATH"; fi &&     if ! [ -z ${SPARK_EXECUTOR_EXTRA_CLASSPATH+x} ]; then SPARK_CLASSPATH="$SPARK_EXECUTOR_EXTRA_CLASSPATH:$SPARK_CLASSPATH"; fi &&     if ! [ -z ${SPARK_MOUNTED_FILES_DIR+x} ]; then cp -R "$SPARK_MOUNTED_FILES_DIR/." .; fi &&     if ! [ -z ${SPARK_MOUNTED_FILES_FROM_SECRET_DIR+x} ]; then cp -R "$SPARK_MOUNTED_FILES_FROM_SECRET_DIR/." .; fi &&     ${JAVA_HOME}/bin/java "${SPARK_EXECUTOR_JAVA_OPTS[@]}" -Dspark.executor.port=$SPARK_EXECUTOR_PORT -Xms$SPARK_EXECUTOR_MEMORY -Xmx$SPARK_EXECUTOR_MEMORY -cp $SPARK_CLASSPATH org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url $SPARK_DRIVER_URL --executor-id $SPARK_EXECUTOR_ID --cores $SPARK_EXECUTOR_CORES --app-id $SPARK_APPLICATION_ID --hostname $SPARK_EXECUTOR_POD_IP
     ---> Running in 2ccd0b0699d0
     ---> a78e8b0add8b
    Removing intermediate container 2ccd0b0699d0
    Successfully built a78e8b0add8b
    Sending build context to Docker daemon 237.4 MB
    Step 1 : FROM spark-base
     ---> def795a30f03
    Step 2 : COPY examples /opt/spark/examples
     ---> Using cache
     ---> eb1a94f3b961
    Step 3 : CMD SPARK_CLASSPATH="${SPARK_HOME}/jars/*" &&     env | grep SPARK_JAVA_OPT_ | sed 's/[^=]*=\(.*\)/\1/g' > /tmp/java_opts.txt &&     readarray -t SPARK_EXECUTOR_JAVA_OPTS < /tmp/java_opts.txt &&     if ! [ -z ${SPARK_MOUNTED_CLASSPATH}+x} ]; then SPARK_CLASSPATH="$SPARK_MOUNTED_CLASSPATH:$SPARK_CLASSPATH"; fi &&     if ! [ -z ${SPARK_EXECUTOR_EXTRA_CLASSPATH+x} ]; then SPARK_CLASSPATH="$SPARK_EXECUTOR_EXTRA_CLASSPATH:$SPARK_CLASSPATH"; fi &&     if ! [ -z ${SPARK_EXTRA_CLASSPATH+x} ]; then SPARK_CLASSPATH="$SPARK_EXTRA_CLASSPATH:$SPARK_CLASSPATH"; fi &&     if ! [ -z ${SPARK_MOUNTED_FILES_DIR+x} ]; then cp -R "$SPARK_MOUNTED_FILES_DIR/." .; fi &&     if ! [ -z ${SPARK_MOUNTED_FILES_FROM_SECRET_DIR+x} ]; then cp -R "$SPARK_MOUNTED_FILES_FROM_SECRET_DIR/." .; fi &&     ${JAVA_HOME}/bin/java "${SPARK_EXECUTOR_JAVA_OPTS[@]}" -Dspark.executor.port=$SPARK_EXECUTOR_PORT -Xms$SPARK_EXECUTOR_MEMORY -Xmx$SPARK_EXECUTOR_MEMORY -cp $SPARK_CLASSPATH org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url $SPARK_DRIVER_URL --executor-id $SPARK_EXECUTOR_ID --cores $SPARK_EXECUTOR_CORES --app-id $SPARK_APPLICATION_ID --hostname $SPARK_EXECUTOR_POD_IP
     ---> Running in df5dd415a4c7
     ---> f13cde19e3d6
    Removing intermediate container df5dd415a4c7
    Successfully built f13cde19e3d6
    You have new mail in /var/spool/mail/root
    [root@sparkonk8s-46929-p4jdl spark-2.2.0-k8s-0.5.0-bin-2.7.3]#
