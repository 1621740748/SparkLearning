{"log":"++ id -u\n","stream":"stderr","time":"2018-03-09T01:30:58.663650812Z"}
{"log":"+ myuid=0\n","stream":"stderr","time":"2018-03-09T01:30:58.674090937Z"}
{"log":"++ id -g\n","stream":"stderr","time":"2018-03-09T01:30:58.674115045Z"}
{"log":"+ mygid=0\n","stream":"stderr","time":"2018-03-09T01:30:58.674769146Z"}
{"log":"++ getent passwd 0\n","stream":"stderr","time":"2018-03-09T01:30:58.674789711Z"}
{"log":"+ uidentry=root:x:0:0:root:/root:/bin/ash\n","stream":"stderr","time":"2018-03-09T01:30:58.676837092Z"}
{"log":"+ '[' -z root:x:0:0:root:/root:/bin/ash ']'\n","stream":"stderr","time":"2018-03-09T01:30:58.676857541Z"}
{"log":"+ /sbin/tini -s -- /bin/sh -c 'SPARK_CLASSPATH=\"${SPARK_HOME}/jars/*\" \u0026\u0026     env | grep SPARK_JAVA_OPT_ | sed '\\''s/[^=]*=\\(.*\\)/\\1/g'\\'' \u003e /tmp/java_opts.txt \u0026\u0026     readarray -t SPARK_DRIVER_JAVA_OPTS \u003c /tmp/java_opts.txt \u0026\u0026     if ! [ -z ${SPARK_MOUNTED_CLASSPATH+x} ]; then SPARK_CLASSPATH=\"$SPARK_MOUNTED_CLASSPATH:$SPARK_CLASSPATH\"; fi \u0026\u0026     if ! [ -z ${SPARK_SUBMIT_EXTRA_CLASSPATH+x} ]; then SPARK_CLASSPATH=\"$SPARK_SUBMIT_EXTRA_CLASSPATH:$SPARK_CLASSPATH\"; fi \u0026\u0026     if ! [ -z ${SPARK_EXTRA_CLASSPATH+x} ]; then SPARK_CLASSPATH=\"$SPARK_EXTRA_CLASSPATH:$SPARK_CLASSPATH\"; fi \u0026\u0026     if ! [ -z ${SPARK_MOUNTED_FILES_DIR+x} ]; then cp -R \"$SPARK_MOUNTED_FILES_DIR/.\" .; fi \u0026\u0026     if ! [ -z ${SPARK_MOUNTED_FILES_FROM_SECRET_DIR} ]; then cp -R \"$SPARK_MOUNTED_FILES_FROM_SECRET_DIR/.\" .; fi \u0026\u0026     ${JAVA_HOME}/bin/java \"${SPARK_DRIVER_JAVA_OPTS[@]}\" -cp $SPARK_CLASSPATH -Xms$SPARK_DRIVER_MEMORY -Xmx$SPARK_DRIVER_MEMORY -Dspark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS $SPARK_DRIVER_CLASS $SPARK_DRIVER_ARGS'\n","stream":"stderr","time":"2018-03-09T01:30:58.676864548Z"}
{"log":"2018-03-09 01:31:00 INFO  SparkContext:54 - Running Spark version 2.2.0-k8s-0.5.0\n","stream":"stdout","time":"2018-03-09T01:31:00.952030448Z"}
{"log":"2018-03-09 01:31:01 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","stream":"stdout","time":"2018-03-09T01:31:01.66921886Z"}
{"log":"2018-03-09 01:31:01 WARN  SparkConf:66 - In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).\n","stream":"stdout","time":"2018-03-09T01:31:01.891946364Z"}
{"log":"2018-03-09 01:31:01 INFO  SparkContext:54 - Submitted application: Spark Pi\n","stream":"stdout","time":"2018-03-09T01:31:01.986764425Z"}
{"log":"2018-03-09 01:31:02 INFO  SecurityManager:54 - Changing view acls to: root\n","stream":"stdout","time":"2018-03-09T01:31:02.017493064Z"}
{"log":"2018-03-09 01:31:02 INFO  SecurityManager:54 - Changing modify acls to: root\n","stream":"stdout","time":"2018-03-09T01:31:02.018444367Z"}
{"log":"2018-03-09 01:31:02 INFO  SecurityManager:54 - Changing view acls groups to: \n","stream":"stdout","time":"2018-03-09T01:31:02.019466245Z"}
{"log":"2018-03-09 01:31:02 INFO  SecurityManager:54 - Changing modify acls groups to: \n","stream":"stdout","time":"2018-03-09T01:31:02.020420035Z"}
{"log":"2018-03-09 01:31:02 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n","stream":"stdout","time":"2018-03-09T01:31:02.021444927Z"}
{"log":"2018-03-09 01:31:02 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 7078.\n","stream":"stdout","time":"2018-03-09T01:31:02.464043938Z"}
{"log":"2018-03-09 01:31:02 INFO  SparkEnv:54 - Registering MapOutputTracker\n","stream":"stdout","time":"2018-03-09T01:31:02.494948887Z"}
{"log":"2018-03-09 01:31:02 INFO  SparkEnv:54 - Registering BlockManagerMaster\n","stream":"stdout","time":"2018-03-09T01:31:02.556421359Z"}
{"log":"2018-03-09 01:31:02 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n","stream":"stdout","time":"2018-03-09T01:31:02.5587841Z"}
{"log":"2018-03-09 01:31:02 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\n","stream":"stdout","time":"2018-03-09T01:31:02.559176084Z"}
{"log":"2018-03-09 01:31:02 INFO  DiskBlockManager:54 - Created local directory at /mnt/tmp/spark-local/spark-8d9572ff-0aa0-4602-8e50-0894d3905f15/blockmgr-8b6b10d8-b838-43a2-9269-da259472c2ba\n","stream":"stdout","time":"2018-03-09T01:31:02.573993067Z"}
{"log":"2018-03-09 01:31:02 INFO  MemoryStore:54 - MemoryStore started with capacity 114.6 MB\n","stream":"stdout","time":"2018-03-09T01:31:02.622597626Z"}
{"log":"2018-03-09 01:31:02 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\n","stream":"stdout","time":"2018-03-09T01:31:02.691594377Z"}
{"log":"2018-03-09 01:31:02 INFO  log:192 - Logging initialized @3996ms\n","stream":"stdout","time":"2018-03-09T01:31:02.814207731Z"}
{"log":"2018-03-09 01:31:02 INFO  Server:345 - jetty-9.3.z-SNAPSHOT\n","stream":"stdout","time":"2018-03-09T01:31:02.885894101Z"}
{"log":"2018-03-09 01:31:02 INFO  Server:403 - Started @4085ms\n","stream":"stdout","time":"2018-03-09T01:31:02.902508883Z"}
{"log":"2018-03-09 01:31:02 INFO  AbstractConnector:270 - Started ServerConnector@7ad546e4{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n","stream":"stdout","time":"2018-03-09T01:31:02.92824121Z"}
{"log":"2018-03-09 01:31:02 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.\n","stream":"stdout","time":"2018-03-09T01:31:02.928269155Z"}
{"log":"2018-03-09 01:31:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3af17be2{/jobs,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:02.965880565Z"}
{"log":"2018-03-09 01:31:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@40dd3977{/jobs/json,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:02.971330569Z"}
{"log":"2018-03-09 01:31:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6a1d204a{/jobs/job,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:02.971939018Z"}
{"log":"2018-03-09 01:31:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6fff253c{/jobs/job/json,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:02.974538129Z"}
{"log":"2018-03-09 01:31:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@591e58fa{/stages,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:02.976314266Z"}
{"log":"2018-03-09 01:31:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2f94c4db{/stages/json,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:02.980157559Z"}
{"log":"2018-03-09 01:31:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@72ccd81a{/stages/stage,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:02.980781642Z"}
{"log":"2018-03-09 01:31:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5d25e6bb{/stages/stage/json,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:02.981890508Z"}
{"log":"2018-03-09 01:31:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@9d157ff{/stages/pool,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:02.982513586Z"}
{"log":"2018-03-09 01:31:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5df417a7{/stages/pool/json,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:02.986412266Z"}
{"log":"2018-03-09 01:31:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7f69d591{/storage,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:02.987080341Z"}
{"log":"2018-03-09 01:31:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1cb3ec38{/storage/json,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:02.990782548Z"}
{"log":"2018-03-09 01:31:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@71c5b236{/storage/rdd,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:02.991482092Z"}
{"log":"2018-03-09 01:31:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2f7a7219{/storage/rdd/json,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:02.992171597Z"}
{"log":"2018-03-09 01:31:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3a1d593e{/environment,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:02.993875226Z"}
{"log":"2018-03-09 01:31:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@361c294e{/environment/json,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:02.994547203Z"}
{"log":"2018-03-09 01:31:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@285d851a{/executors,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:02.99522144Z"}
{"log":"2018-03-09 01:31:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@664a9613{/executors/json,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:03.000099968Z"}
{"log":"2018-03-09 01:31:03 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@15a902e7{/executors/threadDump,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:03.000776876Z"}
{"log":"2018-03-09 01:31:03 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4a3e3e8b{/executors/threadDump/json,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:03.001447935Z"}
{"log":"2018-03-09 01:31:03 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@71104a4{/static,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:03.016093932Z"}
{"log":"2018-03-09 01:31:03 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@47874b25{/,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:03.016921561Z"}
{"log":"2018-03-09 01:31:03 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2c177f9e{/api,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:03.017931241Z"}
{"log":"2018-03-09 01:31:03 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6fefce9e{/jobs/job/kill,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:03.020382913Z"}
{"log":"2018-03-09 01:31:03 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1bdf8190{/stages/stage/kill,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:03.021079791Z"}
{"log":"2018-03-09 01:31:03 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://spark-pi-1520559055810-driver-svc.default.svc.cluster.local:4040\n","stream":"stdout","time":"2018-03-09T01:31:03.030625352Z"}
{"log":"2018-03-09 01:31:03 INFO  SparkContext:54 - Added JAR /opt/spark/examples/jars/spark-examples_2.11-2.2.0-k8s-0.5.0.jar at spark://spark-pi-1520559055810-driver-svc.default.svc.cluster.local:7078/jars/spark-examples_2.11-2.2.0-k8s-0.5.0.jar with timestamp 1520559063064\n","stream":"stdout","time":"2018-03-09T01:31:03.066252887Z"}
{"log":"2018-03-09 01:31:03 WARN  KubernetesClusterManager:66 - The executor's init-container config map was not specified. Executors will therefore not attempt to fetch remote or submitted dependencies.\n","stream":"stdout","time":"2018-03-09T01:31:03.178963972Z"}
{"log":"2018-03-09 01:31:03 WARN  KubernetesClusterManager:66 - The executor's init-container config map key was not specified. Executors will therefore not attempt to fetch remote or submitted dependencies.\n","stream":"stdout","time":"2018-03-09T01:31:03.183703161Z"}
{"log":"2018-03-09 01:31:04 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.\n","stream":"stdout","time":"2018-03-09T01:31:04.992221327Z"}
{"log":"2018-03-09 01:31:04 INFO  NettyBlockTransferService:54 - Server created on spark-pi-1520559055810-driver-svc.default.svc.cluster.local:7079\n","stream":"stdout","time":"2018-03-09T01:31:04.993112743Z"}
{"log":"2018-03-09 01:31:04 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n","stream":"stdout","time":"2018-03-09T01:31:04.994926503Z"}
{"log":"2018-03-09 01:31:04 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, spark-pi-1520559055810-driver-svc.default.svc.cluster.local, 7079, None)\n","stream":"stdout","time":"2018-03-09T01:31:04.997169244Z"}
{"log":"2018-03-09 01:31:05 INFO  BlockManagerMasterEndpoint:54 - Registering block manager spark-pi-1520559055810-driver-svc.default.svc.cluster.local:7079 with 114.6 MB RAM, BlockManagerId(driver, spark-pi-1520559055810-driver-svc.default.svc.cluster.local, 7079, None)\n","stream":"stdout","time":"2018-03-09T01:31:05.004066714Z"}
{"log":"2018-03-09 01:31:05 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, spark-pi-1520559055810-driver-svc.default.svc.cluster.local, 7079, None)\n","stream":"stdout","time":"2018-03-09T01:31:05.024361532Z"}
{"log":"2018-03-09 01:31:05 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, spark-pi-1520559055810-driver-svc.default.svc.cluster.local, 7079, None)\n","stream":"stdout","time":"2018-03-09T01:31:05.027639307Z"}
{"log":"2018-03-09 01:31:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@30c0d731{/metrics/json,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:05.078816743Z"}
{"log":"2018-03-09 01:31:06 INFO  KubernetesClusterSchedulerBackend:54 - Requesting a new executor, total executors is now 1\n","stream":"stdout","time":"2018-03-09T01:31:06.142644222Z"}
{"log":"2018-03-09 01:31:34 INFO  KubernetesClusterSchedulerBackend:54 - SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\n","stream":"stdout","time":"2018-03-09T01:31:34.076798495Z"}
{"log":"2018-03-09 01:31:34 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/spark/work-dir/spark-warehouse').\n","stream":"stdout","time":"2018-03-09T01:31:34.163922671Z"}
{"log":"2018-03-09 01:31:34 INFO  SharedState:54 - Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.\n","stream":"stdout","time":"2018-03-09T01:31:34.165951501Z"}
{"log":"2018-03-09 01:31:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7b78ed6a{/SQL,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:34.173639943Z"}
{"log":"2018-03-09 01:31:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6ec65b5e{/SQL/json,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:34.174333869Z"}
{"log":"2018-03-09 01:31:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@31edeac{/SQL/execution,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:34.175434683Z"}
{"log":"2018-03-09 01:31:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@45bb2aa1{/SQL/execution/json,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:34.18251418Z"}
{"log":"2018-03-09 01:31:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@74db12c2{/static/sql,null,AVAILABLE,@Spark}\n","stream":"stdout","time":"2018-03-09T01:31:34.185777332Z"}
{"log":"2018-03-09 01:31:35 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\n","stream":"stdout","time":"2018-03-09T01:31:35.155280703Z"}
{"log":"2018-03-09 01:31:35 INFO  SparkContext:54 - Starting job: reduce at SparkPi.scala:38\n","stream":"stdout","time":"2018-03-09T01:31:35.308831981Z"}
{"log":"2018-03-09 01:31:35 INFO  DAGScheduler:54 - Got job 0 (reduce at SparkPi.scala:38) with 10 output partitions\n","stream":"stdout","time":"2018-03-09T01:31:35.334797379Z"}
{"log":"2018-03-09 01:31:35 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (reduce at SparkPi.scala:38)\n","stream":"stdout","time":"2018-03-09T01:31:35.335253663Z"}
{"log":"2018-03-09 01:31:35 INFO  DAGScheduler:54 - Parents of final stage: List()\n","stream":"stdout","time":"2018-03-09T01:31:35.336490029Z"}
{"log":"2018-03-09 01:31:35 INFO  DAGScheduler:54 - Missing parents: List()\n","stream":"stdout","time":"2018-03-09T01:31:35.338852441Z"}
{"log":"2018-03-09 01:31:35 INFO  DAGScheduler:54 - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34), which has no missing parents\n","stream":"stdout","time":"2018-03-09T01:31:35.345844025Z"}
{"log":"2018-03-09 01:31:35 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 1832.0 B, free 114.6 MB)\n","stream":"stdout","time":"2018-03-09T01:31:35.528860604Z"}
{"log":"2018-03-09 01:31:35 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 1172.0 B, free 114.6 MB)\n","stream":"stdout","time":"2018-03-09T01:31:35.561103152Z"}
{"log":"2018-03-09 01:31:35 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on spark-pi-1520559055810-driver-svc.default.svc.cluster.local:7079 (size: 1172.0 B, free: 114.6 MB)\n","stream":"stdout","time":"2018-03-09T01:31:35.566726627Z"}
{"log":"2018-03-09 01:31:35 INFO  SparkContext:54 - Created broadcast 0 from broadcast at DAGScheduler.scala:1006\n","stream":"stdout","time":"2018-03-09T01:31:35.577431193Z"}
{"log":"2018-03-09 01:31:35 INFO  DAGScheduler:54 - Submitting 10 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n","stream":"stdout","time":"2018-03-09T01:31:35.60445895Z"}
{"log":"2018-03-09 01:31:35 INFO  KubernetesTaskSchedulerImpl:54 - Adding task set 0.0 with 10 tasks\n","stream":"stdout","time":"2018-03-09T01:31:35.605590628Z"}
{"log":"2018-03-09 01:31:50 WARN  KubernetesTaskSchedulerImpl:66 - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","stream":"stdout","time":"2018-03-09T01:31:50.621784921Z"}
{"log":"2018-03-09 01:32:05 WARN  KubernetesTaskSchedulerImpl:66 - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","stream":"stdout","time":"2018-03-09T01:32:05.621154847Z"}
{"log":"2018-03-09 01:32:20 WARN  KubernetesTaskSchedulerImpl:66 - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","stream":"stdout","time":"2018-03-09T01:32:20.621454828Z"}
{"log":"2018-03-09 01:32:35 WARN  KubernetesTaskSchedulerImpl:66 - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","stream":"stdout","time":"2018-03-09T01:32:35.620841039Z"}
{"log":"2018-03-09 01:32:50 WARN  KubernetesTaskSchedulerImpl:66 - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","stream":"stdout","time":"2018-03-09T01:32:50.621173073Z"}
{"log":"2018-03-09 01:33:05 WARN  KubernetesTaskSchedulerImpl:66 - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","stream":"stdout","time":"2018-03-09T01:33:05.620520313Z"}
{"log":"2018-03-09 01:33:20 WARN  KubernetesTaskSchedulerImpl:66 - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","stream":"stdout","time":"2018-03-09T01:33:20.620900009Z"}
{"log":"2018-03-09 01:33:35 WARN  KubernetesTaskSchedulerImpl:66 - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","stream":"stdout","time":"2018-03-09T01:33:35.621266484Z"}
{"log":"2018-03-09 01:33:50 WARN  KubernetesTaskSchedulerImpl:66 - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","stream":"stdout","time":"2018-03-09T01:33:50.620570357Z"}
